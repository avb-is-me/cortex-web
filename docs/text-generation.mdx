---
title: Text Generation
description: Text Generation.
slug: "text-generation"
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

:::warning
ðŸš§ Cortex is under construction.
:::

Cortex's Chat API is compatible with OpenAIâ€™s [Chat Completions](https://platform.openai.com/docs/api-reference/chat) endpoint. It is a drop-in replacement for local inference.

For local inference, Cortex is [multi-engine](#multiple-local-engines) and supports the following model formats:

- `GGUF`: A generalizable LLM format that runs across CPUs and GPUs. Cortex implements a GGUF runtime through [llama.cpp](https://github.com/ggerganov/llama.cpp/).
- `TensorRT`: A a production-ready, enterprise-grade LLM format optimized for fast inference on NVIDIA GPUs. Cortex implements a TensorRT runtime through [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM).

For remote inference, Cortex routes requests to multiple APIs, while providing a single, easy to use, OpenAI compatible endpoint.

## Usage

<Tabs>
  <TabItem  value="CLI" label="CLI" default>
    ```bash 
    # Streaming 
    cortex chat --model janhq/TinyLlama-1.1B-Chat-v1.0-GGUF
    ```
  </TabItem>
  <TabItem value="Javascript" label="Javascript">
  </TabItem>
  <TabItem value="CURL" label="CURL">
  </TabItem>
</Tabs>

**Read more:**

- Chat Completion Object
- Chat Completions API
- Chat Completions CLI

## Capabilities

### Multiple Local Engines

Cortex scales applications from prototype to production. It runs on CPU-only laptops with Llama.cpp and GPU-accelerated clusters with TensorRT-LLM.

To learn more about how to configure each engine:

- Use llama.cpp
- Use tensorrt-llm

To learn more about our engine architecture:

- cortex.cpp
- cortex.llamacpp
- cortex.tensorRTLLM

### Multiple Remote APIs

Cortex also works as an aggregator to make remote inference requests from a single endpoint.

Currently, Cortex supports:

- OpenAI
- Groq
- Cohere
- Anthropic
- MistralAI
- Martian
- OpenRouter

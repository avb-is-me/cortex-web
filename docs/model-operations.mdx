---
title: Model Operations
description: Model Operations
slug: "model-operations"
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

:::warning
ðŸš§ Cortex is under construction.
:::

Cortex's Chat API is compatible with OpenAIâ€™s [Models](https://platform.openai.com/docs/api-reference/models) endpoint. It is a drop-in replacement for model management.

In addition to the [Model API](https://platform.openai.com/docs/api-reference/models), Cortex exposes lower level operations for managing models like downloading models from a model hub and model loading.

## Usage

<Tabs>
  <TabItem  value="CLI" label="CLI" default>
    ```bash 
    cortex models pull <repo_name-or-filepath>
    cortex models start
    cortex models stop
    ```
  </TabItem>
  <TabItem value="Javascript" label="Javascript">
  </TabItem>
  <TabItem value="CURL" label="CURL">
  ## Load Models
    Cortex provides two ways to load your local model into the Cortex server.

    You can load the model using the CLI or the CURL following command:

      ### Request
      To load a model, use the following in your request:
      ```curl
        curl -X POST "http://localhost:1337/models/{janhq/TinyLlama-1.1B-Chat-v1.0-GGUF}/start" \
        -H "Content-Type: application/json" \
        -d '{
        "ctx_len": 512,
        "ngl": 4,
        "embedding": true,
        "n_parallel": 2,
        "cpu_threads": 8,
        "prompt_template": "Hello, this is a predefined text",
        "system_prompt": "System prompt text",
        "ai_prompt": "AI specific prompt text",
        "user_prompt": "User provided prompt text",
        "llama_model_path": "/path/to/llama/model",
        "mmproj": "Projection matrix details",
        "cont_batching": true,
        "vision_model": false,
        "text_model": true
        }'



      ```
      ### Endpoint Response
      Below are examples of the `start model` endpoint response from the Cortex server:
      ```curl
    {
      "message": "Model started successfully",
      "modelId": "janhq/TinyLlama-1.1B-Chat-v1.0-GGUF"
    }

      ```

    ## Unload Models
    Cortex provides two ways to unload your local model from the Cortex server.

    You can unload the model using the CLI or the CURL following command:

      ### Request
      To unload a model, use the following in your request:
      ```curl
        curl --request POST \
        --url http://localhost:1337/models/janhq/TinyLlama-1.1B-Chat-v1.0-GGUF/stop



      ```
      ### Endpoint Response
      Below are examples of the `stop model` endpoint response from the Cortex server:
      ```curl
    {
      "message": "Model stopped successfully",
      "modelId": "janhq/TinyLlama-1.1B-Chat-v1.0-GGUF"
    }

      ```
  </TabItem>
</Tabs>

## Capabilities

### Model Hub

Cortex is compatible with 3rd party model hubs like [Hugging Face Hub](https://huggingface.co/models). 

To make the experience easier, Cortex has curated a list of models and their configurations.

**Available hubs**
- https://huggingface.co/cortexhub 


### Model Settings

Cortex looks for a `model.yaml` to run models. Otherwise it is generated from model metadata. The yaml files are flat, portable and contains all the parameters needed to run models.

**Example `model.yaml`**
```yaml
# Cortex state
name: llama3
version: 1
files: [ ... ]

# Markup Language
prompt_template: ...
pre_prompt: ...
stop: ...

# Results Preferences
top_p: ...
temperature: ...
frequency_penalty: ...
presence_penalty: ...
max_tokens: ...
stream: true

# Engine Settings
ngl: ...
ctx_len: ...
n_parallel: ...
cpu_threads: ...
engine: cortex.llamacpp 
```

### Model Settings Templates

Cortex provides a small set of model settings templates for popular model architectures, like `ChatML`.

You can apply the templates to any model.

:::note
Learn more about Models capabilities:
- [Model Object](/api-reference#tag/models/get/models/{id})
- [Model API](/api-reference#tag/models/post/models)
- [Model CLI](/docs/cli/models/)
:::
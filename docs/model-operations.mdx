---
title: Model Operations
description: Model Operations
slug: "model-operations"
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

:::warning
ðŸš§ Cortex is under construction.
:::

Cortex's Chat API is compatible with OpenAIâ€™s [Models API](https://platform.openai.com/docs/api-reference/models) endpoint. It is a drop-in replacement for model management. Additionally, Cortex exposes lower level operations for managing models like downloading models from a model hub and model loading.

Cortex also supports multiple model formats and architectures, including `GGUF`, `ONNX`, and `TensorRT-LLM`.

## Usage

<Tabs>
  <TabItem  value="CLI" label="CLI" default>
    ```bash 
    cortex models pull <repo_name-or-filepath>
    cortex models start
    cortex models stop
    ```
  </TabItem>
  <TabItem value="Javascript" label="Javascript">
  </TabItem>
  <TabItem value="CURL" label="CURL">
  ## Load Models
    Cortex provides two ways to load your local model into the Cortex server.

    You can load the model using the CLI or the CURL following command:

      ### Request
      To load a model, use the following in your request:
      ```bash
        curl -X POST "http://localhost:1337/models/{janhq/TinyLlama-1.1B-Chat-v1.0-GGUF}/start" \
        -H "Content-Type: application/json" \
        -d '{
        "ctx_len": 512,
        "ngl": 4,
        "embedding": true,
        "n_parallel": 2,
        "cpu_threads": 8,
        "prompt_template": "Hello, this is a predefined text",
        "system_prompt": "System prompt text",
        "ai_prompt": "AI specific prompt text",
        "user_prompt": "User provided prompt text",
        "llama_model_path": "/path/to/llama/model",
        "mmproj": "Projection matrix details",
        "cont_batching": true,
        "vision_model": false,
        "text_model": true
        }'



      ```
      ### Endpoint Response
      Below are examples of the `start model` endpoint response from the Cortex server:
      ```bash
    {
      "message": "Model started successfully",
      "modelId": "janhq/TinyLlama-1.1B-Chat-v1.0-GGUF"
    }

      ```

    ## Unload Models
    Cortex provides two ways to unload your local model from the Cortex server.

    You can unload the model using the CLI or the CURL following command:

      ### Request
      To unload a model, use the following in your request:
      ```bash
        curl --request POST \
        --url http://localhost:1337/models/janhq/TinyLlama-1.1B-Chat-v1.0-GGUF/stop



      ```
      ### Endpoint Response
      Below are examples of the `stop model` endpoint response from the Cortex server:
      ```bash
    {
      "message": "Model stopped successfully",
      "modelId": "janhq/TinyLlama-1.1B-Chat-v1.0-GGUF"
    }

      ```
  </TabItem>
</Tabs>

## Capabilities

### Model Registry 

Cortex is compatible with 3rd party model hubs like [Hugging Face](https://huggingface.co).

Cortex has a selection of preconfigured models across `GGUF`, `ONNX`, and `TensorRT-LLM` formats. 

The available model variants, across sizes, formats, and quantizations, are stored in each repo's branches.

<Tabs>
  <TabItem  value="GGUF" label="GGUF" default>
    ```bash 
    cortex models pull llama3
    cortex models pull llama3:gguf
    cortex models pull llama3:8B-gguf
    ```
  </TabItem>
  <TabItem value="ONNX" label="ONNX">
    ```bash 
    cortex models pull llama3:onnx
    ```
  </TabItem>
  <TabItem value="TensorRT-LLM" label="TensorRT-LLM">
    
    TensorRT-LLM engines are specific to the hardware you are running on.

    ```bash 
    cortex models pull 8B-tensorrtllm-linux-ada
    cortex models pull tensorrtllm-windows-ada
    ```
  </TabItem>
</Tabs>

### Model Settings

Cortex looks for a `model.yaml` to run models. The `yaml` files are flat, portable, and contain all the runtime parameters.

If a `model.yaml` is not available, Cortex autogenerates it from model metadata.

<Tabs>
  <TabItem  value="GGUF" label="GGUF" default>
    **Example `model.yaml` for GGUF models**

    ```yaml
    # Cortex state
    name: llama3
    version: 1
    files: [ ... ]

    # Markup Language
    prompt_template: ...
    pre_prompt: ...
    stop: ...

    # Results Preferences
    top_p: ...
    temperature: ...
    frequency_penalty: ...
    presence_penalty: ...
    max_tokens: ...
    stream: true

    # Engine Settings
    ngl: ...
    ctx_len: ...
    n_parallel: ...
    cpu_threads: ...
    engine: cortex.llamacpp 
    ```
  </TabItem>
  <TabItem value="ONNX" label="ONNX">
    **Example `model.yaml` for `llama3 8B-onnx`**

    ```yaml
    name: Llama 3
    model: llama3:8B
    version: 1

    files:
      - model_path: model.onnx

    # Results Preferences
    top_p: 0.95
    temperature: 0.7
    frequency_penalty: 0
    presence_penalty: 0
    max_tokens: 8192 # Infer from base config.json -> max_position_embeddings
    stream: true # true | false

    # Engine / Model Settings
    engine: cortex.onnx
    prompt_template: "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
    # Prompt template: Can only be retrieved from instruct model
    # - https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/tokenizer_config.json#L2053
    # - Requires jinja format parser
    ```

  </TabItem>
  <TabItem value="TensorRT-LLM" label="TensorRT-LLM">
  **Example `model.yaml` for `llama3 8B-tensorrt-llm-windows-ada`**
  ```yaml
    name: Llama 3
    model: llama3:8B
    version: 1

    # files:

    # Results Preferences
    top_p: 0.95
    temperature: 0.7
    frequency_penalty: 0
    presence_penalty: 0
    max_tokens: 8192 # Infer from base config.json -> max_position_embeddings
    stream: true # true | false

    # Engine / Model Settings
    engine: cortex.tensorrtllm
    os: windows # from CI env var
    gpu_arch: ada # from CI env var
    quantization_method: awq # from CI env var
    precision: int4 # from CI env var
    tp: 1 # from CI env var
    trtllm_version: 0.9.0 # From CI env var
    ctx_len: 8192 # Infer from base config.json -> max_position_embeddings
    text_model: false # Fixed value - https://github.com/janhq/jan/blob/dev/extensions/tensorrt-llm-extension/resources/models.json#L41C7-L41C26
    prompt_template: "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
    # Prompt template: Can only be retrieved from instruct model
    # - https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/tokenizer_config.json#L2053
    # - Requires jinja format parser
    ```
    **Example `model.yaml` for `llama3 8B-tensorrt-llm-linux-ada`**
    
    ```yaml
    name: Llama 3
    model: llama3:8B
    version: 1

    # files:

    # Results Preferences
    top_p: 0.95
    temperature: 0.7
    frequency_penalty: 0
    presence_penalty: 0
    max_tokens: 8192 # Infer from base config.json -> max_position_embeddings
    stream: true # true | false

    # Engine / Model Settings
    engine: cortex.tensorrtllm
    os: linux # from CI env var
    gpu_arch: ada # from CI env var
    quantization_method: awq # from CI env var
    precision: int4 # from CI env var
    tp: 1 # from CI env var
    trtllm_version: 0.9.0 # From CI env var
    ctx_len: 8192 # Infer from base config.json -> max_position_embeddings
    text_model: false # Fixed value - https://github.com/janhq/jan/blob/dev/extensions/tensorrt-llm-extension/resources/models.json#L41C7-L41C26
    prompt_template: "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
    # Prompt template: Can only be retrieved from instruct model
    # - https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/tokenizer_config.json#L2053
    # - Requires jinja format parser
    ```
  </TabItem>
</Tabs>

### Model Presets

Cortex provides a small set of presets, i.e. templates for model settings, for popular model architectures, like `ChatML`.

You can apply the templates to any model.

:::note
Learn more about Models capabilities:
- [Model Object](/api-reference#tag/models/get/models/{id})
- [Model API](/api-reference#tag/models/post/models)
- [Model CLI](/docs/cli/models/)
:::